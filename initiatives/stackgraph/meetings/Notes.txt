Dec 8, 2025
Discuss Time Travelling Topology
Invited Remco Beckers Bram Schuur Louis Lotter
Attachments Discuss Time Travelling Topology 
Meeting records Transcript File 

Summary
Bram Schuur and Remco Beckers discussed decoupling Stack Graph functionality, prioritizing topology migration—the source of frequent corruption—before settings migration, which impacts about 70% of the codebase and relies on Stack Graph for consistency features. They considered PostgreSQL with Hibernate to maintain settings consistency but recognized ClickHouse's limitations in natively supporting consistency features, which necessitates exploring alternative event handling and schema design for components like health states and topology change events. Louis Lotter, Bram Schuur, and Remco Beckers agreed to create a comprehensive overview document, incorporating Frank's work on RocksDB and Bram Schuur's Kafka streaming idea, to de-risk the project before moving forward with the ClickHouse proof of concept, acknowledging that the primary risks are query performance, time travel efficiency, and data model fit.

Details
Notes Length: Standard
Decoupling Stack Graph Functionality Bram Schuur highlighted the long-standing goal of decoupling Stack Graph functionality, specifically separating settings (configuration items like layers, domains, and metric bindings) from topology. The ultimate aim of this decoupling is to move topology into a separate store (00:00:00). Remco Beckers agreed with this goal (00:01:20).
Challenges in Migrating Settings Remco Beckers identified that settings migration poses a significant challenge because settings are stored in Stack Graph, and Stack Graph's "fancy features" like constraints and transactionality are heavily utilized to ensure a consistent data and domain model (00:01:20). These features, including validating relations and referential integrity, are currently managed by Stack Graph's object mapping and constraints (00:02:16).
Potential Solutions for Settings Consistency The speakers discussed moving the settings logic away from Stack Graph and proposed that switching to PostgreSQL with Hibernate would likely be the easiest route to maintain consistency, as it closely aligns with current practices (00:03:22). Remco Beckers also noted that ClickHouse does not natively support the necessary consistency features, which would require a custom solution or relying on a single writer service with master-master failover for HA setups (00:04:30).
Complexity of Settings Migration and Decoupling Bram Schuur pointed out that the settings are pervasive, affecting about 70% of the codebase, which means migration would be a "huge refactoring". Remco Beckers added that the complexity was previously centered on relations between topology and settings, which has been largely resolved (00:06:33).
Prioritizing Topology vs. Settings Migration Louis Lotter asked if settings migration should be done first, but Remco Beckers and Bram Schuur agreed that topology is the source of most problems, mainly due to it frequently becoming corrupt, whereas settings are written less often (00:07:37). They suggested focusing on topology first, as moving settings and finding topology still broken would be counterproductive (00:08:28).
Impact of Stack Graph Removal on Health States and Events Remco Beckers outlined components built on or integrated into Stack Graph that would be affected by a migration, starting with health states, which should be straightforward to migrate if topology can be successfully moved to a store like ClickHouse. Events, specifically "topology changed events," present a more complex challenge compared to external events that were already planned for migration (00:09:54).
Rethinking Topology Changed Events Remco Beckers explained that the current approach to topology change events relies on Stack Graph, and a new storage solution would necessitate a different meaning for these events, such as indicating an update to component data that might prompt a recalculation (00:11:05). The primary internal use for these events is to optimize the topology query service and event querying. Louis Lotter suggested that if querying is cheap, the need for these complex event-based optimizations might be reduced (00:12:05).
Challenges in Prototyping Event Handling Remco Beckers noted that creating a prototype for event handling with a new store like ClickHouse would require extensive faking of information, as many elements are currently tied to Stack Graph. Bram Schuur suggested the problem lies in an impedance mismatch between the ClickHouse approach and the required incremental processing capabilities, necessitating very careful design (00:14:25).
Current ClickHouse Prototype Implementation Remco Beckers described their current low-level implementation, which tries to mimic Stack Graph APIs but faces challenges because the APIs still rely on Stack Graph for fetching layers and domains. They are currently using a pull loop every 15 seconds instead of an event-based approach to stream data, though this does not solve all Stack Graph dependencies (00:15:51).
Handling Updates and Events in ClickHouse The discussion shifted to how updates and events would be managed in ClickHouse, which typically favors a fire-and-forget write model (00:17:00). Remco Beckers suggested that to handle updates, they might need to write an event to Kafka after a write, indicating a topology change for specific identifiers (00:18:03). These events would be optimistic, requiring database checks for verification, which would change the events' behavior and meaning (00:18:53).
User and Internal Use Cases for Topology Change Events Louis Lotter clarified that topology change events are primarily used for user investigation, such as tracking configuration changes in Kubernetes, rather than triggering other behavior (00:20:38). Remco Beckers clarified that there are also internal technical events, like component updates, used for optimizations such as conditionally running the state service (00:21:35).
Performance and Scalability Concerns with ClickHouse Bram Schuur expressed concern that a raw scan approach in ClickHouse for calculating health states would be proportional to the landscape size, which is undesirable, whereas the current system is optimized for updates (00:21:35). Remco Beckers acknowledged this but highlighted that ClickHouse is smarter than an HBase scan, though it still has limits on how fast it can calculate health states for massive numbers of components (00:22:32).
Query Optimization in ClickHouse Remco Beckers discussed the need for careful schema design in ClickHouse to maintain efficient query performance, especially concerning result set size and data scanned (00:24:17). They mentioned that including the type name in the key helps prune the data ClickHouse needs to examine (00:26:42). Query optimization tooling is robust, including query logs with detailed information and the ability to experiment with different schemas via the command line interface (00:29:11).
Indirect Relations and With Neighbors Traversal Remco Beckers indicated that they had not yet implemented `with neighbors off` or indirect relations in the ClickHouse prototype (00:31:16). They believe indirect relations are a needed feature, likely implemented through analytical approaches like breadth-first search. This feature, if slow, could potentially load topology first and then relations later to make the UI more responsive (00:32:20).
Client Caching and Transactionality Trade-offs Remco Beckers highlighted that HBase's Stack Graph client performs heavy caching due to its transactional system, ensuring data consistency, a feature lacking in the standard ClickHouse client (00:35:27). Bram Schuur noted that ClickHouse's more liberal data format means time-based caching would be necessary, creating a trade-off between latency and cacheability (00:36:45).
Benefits of ClickHouse Over Stack Graph Remco Beckers emphasized several benefits of using ClickHouse: the ability to enter topology data for any time stamp at any time, unlike Stack Graph's strictly monotonically increasing time (00:41:27). It allows capturing "happens time" or "observed time" from the collector, which is more accurate than Stack State's backend ingestion time (00:42:22). Furthermore, ClickHouse offers more high-level SQL features like fuzzy search and pagination, which improve developer experience and troubleshooting (00:43:29).
Rationale for Choosing ClickHouse Louis Lotter questioned why ClickHouse would be chosen over alternatives like PostgreSQL or RoxDB (00:45:54). Remco Beckers reasoned that ClickHouse is already suitable for the kind of data they query, particularly for logs and potentially topology, which lacks a perfect fit solution (00:47:14) (00:49:11). Bram Schuur expressed concern that using a general-purpose database like ClickHouse for topology could repeat past issues experienced with Elastic Search, which was not perfectly suited for metrics and logs (00:48:13).
Deployment Complexity and Resource Constraints Remco Beckers and Louis Lotter agreed that building a custom solution like Honeycomb's is not viable due to limited resources and the need for a self-hosted, on-premise solution (00:51:35). This context necessitates picking a solution that is reasonably easy to run and well-documented, making existing options like ClickHouse an important practical consideration (00:52:38).
Comparison to FoundationDB and RoxDB Remco Beckers explained that FoundationDB is a database foundation requiring significant custom development, whereas they need an out-of-the-box solution (00:53:28). The concept of using RoxDB, possibly implementing Stack Graph on top of it, would again involve building a database from building blocks, which they feel is too much effort (00:55:10). ClickHouse, being optimized for analytical processing and data ingestion, is a better fit for the high volume of topology data compared to transactional systems like PostgreSQL (00:54:19).
Data Model Fit with ClickHouse Remco Beckers outlined the potential risk of using ClickHouse, noting that its roughly predefined data model might not accommodate their data, leading to problems if the required optimizations cannot be utilized. Louis Lotter suggested that a possible alternative involves sacrificing the aspects of their system that rely on incompatible functionality. They also discussed that if queries are too random, ClickHouse might not work for 50% of cases, making it unusable (00:56:24).
Alternative: Kafka Streams with RocksDB Bram Schuur proposed an alternative route using Kafka Streams with RocksDB, where sharding is managed by Kafka, and the system is event-sourced. Each shard would materialize data into RocksDB, with no consensus required since Kafka handles the sharding, and queries would target specific shards based on discovery (00:57:20). Remco Beckers pointed out that the query side would still require a solution, potentially similar to StackGraph, as RocksDB is primarily a key-value store, not natively supporting complex queries (00:58:16).
Querying Approaches and Transactions Remco Beckers contrasted the Kafka Streams/RocksDB approach, which would require implementing custom query APIs or potentially SQL directly, with ClickHouse's already powerful query language (00:59:23). Bram Schuur noted that the Kafka/RocksDB approach, similar to ClickHouse, would be designed without strong transactions, which generally improves resilience against corruption and data loss. Remco Beckers agreed that adapting the domain's concept of identities for components and elements would be a large but necessary undertaking regardless of the chosen storage solution, carrying the risk of making processes like component merging expensive (01:00:39).
Prototyping and Risk Mitigation Louis Lotter advocated for building a hacky prototype to check for high-risk cases and verify the feasibility of critical functionalities. Remco Beckers identified the merging of components and interpolations as high-risk parts not yet covered by the existing ClickHouse version (01:01:34). Remco Beckers detailed the current ClickHouse implementation, which involves replacing the Kafka exporter after the auto mapper and hacking the Xtoposync task to ensure Kubernetes and agent data, including source properties, are written to ClickHouse for testing (01:02:54).
AI Assistance in Refactoring Louis Lotter inquired about the potential help of AI in the rewrite, to which Remco Beckers responded that it significantly aided the proof of concept, particularly with Scala code using Claude. Remco Beckers shared an experience where AI successfully re-implemented the `topology query service` trait to use ClickHouse instead of StackGraph, even helping to identify and address complex input parameters in the code (01:03:48). Louis Lotter noted that refactoring existing code, rather than novel code generation, is currently the most successful use of AI in engineering (01:08:21).
Proof of Concept Scope and Success Criteria Bram Schuur emphasized the importance of defining the success criteria for a proof of concept, especially concerning the time it would take and the chances of success, as Louis Lotter stressed the goal of de-risking the project (01:09:11). Remco Beckers stated that the primary risk is performance during querying, including time travel, which requires further testing. Other success criteria include demonstrating that the new solution is significantly faster for certain queries than the current nightly system and managing storage usage to not exceed two to three times the current amount (01:11:20).
Database Options and Next Steps Louis Lotter concluded that they could not afford multiple proof of concepts (PoCs) and would need to choose the option with the highest chance of success, suggesting they might stay in analysis mode longer if needed (01:13:55). They agreed that the PoC code should be viewed as disposable and not merged into production (01:14:57). Louis Lotter proposed that the next steps involve taking a close look at the work done by Frank on RocksDB, discussing the Kafka streaming event sourcing idea with Bram Schuur, and creating an overview document of the options before moving forward with a ClickHouse PoC (01:16:37) (01:27:05).

Suggested next steps
Remco Beckers will leave the whole setup running for about a week to see if the amount of time they have collected data over affects the query performance.
Louis Lotter will chat again with the group later this week once the overview document is ready.
Louis Lotter will take the recording and notes to create a nice overview document of the discussion.
Bram Schuur will ask Frank a couple of questions about the Rox DB thing.
Louis Lotter, Remco Beckers and Bram Schuur will watch Frank's presentation and chat with him about the Rox DB thing to understand his approach better and share what is learned.

You should review Gemini's notes to make sure they're accurate. Get tips and learn how Gemini takes notes
Please provide feedback about using Gemini to take notes in a short survey.
